{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db89c417-ee26-4eb2-86ff-837c9a7014b1",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ec058-b9de-4664-9805-57153d69c8f9",
   "metadata": {},
   "source": [
    "\n",
    "Polynomial functions and kernel functions in machine learning algorithms are related in the context of feature mapping and the creation of nonlinear decision boundaries.\n",
    "\n",
    "Feature Mapping: Both polynomial functions and kernel functions are used to perform feature mapping. Feature mapping involves transforming the input data from its original space into a higher-dimensional feature space. This allows the data to be represented in a way that makes it easier for a machine learning algorithm to learn complex relationships.\n",
    "\n",
    "Nonlinear Decision Boundaries: In machine learning, linear models are limited to learning linear decision boundaries, which may not be sufficient for capturing complex patterns in the data. Polynomial functions and kernel functions enable the creation of nonlinear decision boundaries by mapping the data to a higher-dimensional space.\n",
    "\n",
    "Polynomial Functions: Polynomial functions are used to transform the original input features into polynomial terms. For example, a polynomial function of degree 2 can transform a 2-dimensional input (x, y) into a 6-dimensional feature vector (1, x, y, x^2, xy, y^2). By using these higher-order terms, polynomial functions can capture nonlinear relationships between the input features.\n",
    "\n",
    "Kernel Functions: Kernel functions are a generalization of feature mapping, allowing for more flexible and efficient computations. They implicitly map the data into a higher-dimensional feature space without explicitly computing the transformed features. Kernel functions measure the similarity between pairs of data points in the original space and can be used in various machine learning algorithms, such as Support Vector Machines (SVMs). The use of kernel functions enables learning nonlinear decision boundaries in a computationally efficient manner.\n",
    "\n",
    "In summary, polynomial functions and kernel functions are related in their ability to perform feature mapping and enable the creation of nonlinear decision boundaries in machine learning algorithms. Polynomial functions explicitly transform the input features into higher-order terms, while kernel functions provide a more efficient and implicit way to achieve the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46bacc-fdf7-42a6-aefc-0af1a451503a",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2a228d-8d76-4f70-a379-26a315db4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # degree is the degree of the polynomial kernel\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f2431-bbb0-4e76-88c0-45f53e909335",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e1333-b5df-4c5b-8406-24e2afabcafe",
   "metadata": {},
   "source": [
    "\n",
    "In Support Vector Regression (SVR), epsilon (Îµ) is a hyperparameter that controls the width of the margin and the tolerance for errors. Increasing the value of epsilon affects the number of support vectors in the following way:\n",
    "\n",
    "Larger Epsilon: When the value of epsilon is increased, it expands the margin around the predicted function. A larger margin allows more training data points to be within the margin without incurring any penalty. Consequently, more data points can be classified as support vectors. This is because a larger epsilon allows for a greater degree of error tolerance, allowing more data points to be included within the margin.\n",
    "\n",
    "More Support Vectors: Support vectors are the data points that lie on or within the margin or violate the margin. As epsilon increases, the margin widens, and the algorithm becomes more tolerant to errors. This increased tolerance can result in a larger number of data points falling within the margin and becoming support vectors. Therefore, increasing the value of epsilon typically leads to an increase in the number of support vectors.\n",
    "\n",
    "It's important to note that the number of support vectors can also depend on other factors, such as the complexity of the dataset, the choice of the kernel function, and the regularization parameter (C) used in SVR. These factors interact with epsilon to determine the final number of support vectors in SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42213c52-9520-4316-9e27-dffcb5e5983a",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9785a3-f5c4-4e74-8b70-2d5ea18ee6ce",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a supervised learning algorithm that is used for regression tasks. The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in SVR can significantly affect its performance. Here's a breakdown of each parameter and its impact:\n",
    "\n",
    "Kernel Function:\n",
    "The kernel function is responsible for mapping the input data into a higher-dimensional feature space. It determines the shape of the decision boundary in SVR. Different kernel functions have different properties, and the choice depends on the characteristics of the data. Some commonly used kernel functions are:\n",
    "Linear: This is the simplest kernel and creates a linear decision boundary. It is suitable when the data is linearly separable.\n",
    "Polynomial: This kernel introduces non-linearity using polynomial functions.\n",
    "Radial Basis Function (RBF): This is the most widely used kernel function. It creates non-linear decision boundaries and is suitable for data with complex relationships.\n",
    "The choice of the kernel function depends on the underlying patterns in the data. If the data has complex relationships, a non-linear kernel like RBF can capture those patterns better. However, if the data is simple and linearly separable, a linear kernel may be sufficient.\n",
    "\n",
    "C Parameter:\n",
    "The C parameter in SVR determines the trade-off between minimizing the training error and allowing deviations from the training data points. It controls the regularization of the model. A smaller value of C emphasizes a larger margin and allows more deviations, while a larger value of C focuses on minimizing errors and reducing deviations.\n",
    "Increasing the C parameter may lead to overfitting, as the model becomes more sensitive to individual data points and tries to fit them precisely. On the other hand, decreasing the C parameter may result in underfitting, as the model becomes too flexible and may fail to capture the underlying patterns in the data.\n",
    "\n",
    "Epsilon Parameter:\n",
    "The epsilon parameter defines the margin around the predicted value within which errors are considered acceptable. It determines the width of the epsilon-tube, or the insensitive zone, around the predicted values. SVR aims to fit the training data within this tube. Any predictions falling within the tube are considered accurate, while those outside the tube are penalized.\n",
    "Increasing the epsilon parameter widens the insensitive zone and allows more errors to be tolerated. This can lead to a larger margin and a more robust model against outliers. Decreasing the epsilon parameter makes the model less tolerant to errors, resulting in a smaller margin and potentially better accuracy on training data.\n",
    "\n",
    "Gamma Parameter:\n",
    "The gamma parameter defines the influence of a single training example and affects the smoothness of the decision boundary. It determines the reach of each training example, with low values meaning 'far' and high values meaning 'close.' A small gamma makes the decision boundary more smooth, while a large gamma makes it more complex and wiggly, potentially leading to overfitting.\n",
    "Increasing the gamma parameter can make the model more prone to overfitting, especially when the data is noisy or has many outliers. Decreasing the gamma parameter can lead to a simpler decision boundary, which may be desirable when the data is less complex or when the model is overfitting.\n",
    "\n",
    "It's important to note that the impact of these parameters may vary depending on the dataset and the specific problem. It is recommended to perform cross-validation or use grid search techniques to find the optimal values for these parameters.\n",
    "\n",
    "Overall, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter in SVR requires careful consideration, and their values should be tuned based on the characteristics of the data and the desired trade-offs between accuracy, flexibility, and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1eca2-31cf-42b8-9121-e0aa5dce4c06",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c1b7aa-49d8-4c3e-ac77-b689f327a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the necessary libraries and load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# 2. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 3. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Preprocess the data (scaling in this example)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Create an instance of the SVC classifier and train it\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "\n",
    "# 7. Evaluate the performance of the classifier (accuracy in this example)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# 8. Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# 9. Train the tuned classifier on the entire dataset\n",
    "X_scaled = scaler.transform(X)\n",
    "best_classifier.fit(X_scaled, y)\n",
    "\n",
    "# 10. Save the trained classifier to a file\n",
    "filename = 'trained_classifier.sav'\n",
    "pickle.dump(best_classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da55d98-8bda-4762-ac1e-d1c0fd3ab612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
